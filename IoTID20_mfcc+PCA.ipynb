{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_normal = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_normal.csv')\n",
    "train_one = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_1.csv')\n",
    "train_two = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_2.csv')\n",
    "train_three = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_3.csv')\n",
    "train_four = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_4.csv')\n",
    "train_five = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_5.csv')\n",
    "train_six = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_6.csv')\n",
    "train_seven = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_7.csv')\n",
    "train_eight = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_8.csv')\n",
    "\n",
    "test_normal = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_normal.csv')\n",
    "test_one = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_1.csv')\n",
    "test_two = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_2.csv')\n",
    "test_three = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_3.csv')\n",
    "test_four = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_4.csv')\n",
    "test_five = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_5.csv')\n",
    "test_six = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_6.csv')\n",
    "test_seven = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_7.csv')\n",
    "test_eight = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal['label']=0\n",
    "train_one['label']=1\n",
    "train_two['label']=2\n",
    "train_three['label']=3\n",
    "train_four['label']=4\n",
    "train_five['label']=5\n",
    "train_six['label']=6\n",
    "train_seven['label']=7\n",
    "train_eight['label']=8\n",
    "\n",
    "test_normal['label']=0\n",
    "test_one['label']=1\n",
    "test_two['label']=2\n",
    "test_three['label']=3\n",
    "test_four['label']=4\n",
    "test_five['label']=5\n",
    "test_six['label']=6\n",
    "test_seven['label']=7\n",
    "test_eight['label']=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge = pd.concat([train_normal, train_one, train_two, train_three, train_four, train_five, train_six, train_seven, train_eight], ignore_index=True)\n",
    "test_merge = pd.concat([test_normal, test_one, test_two, test_three, test_four, test_five, test_six, test_seven, test_eight], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values  # 마지막 열을 제외한 데이터\n",
    "        self.labels = dataframe.iloc[:, -1].values  # 마지막 열을 라벨 데이터로 사용\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': torch.tensor(self.data[idx], dtype=torch.float32),\n",
    "                  'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(train_merge.iloc[:, :-1].values)\n",
    "X_test_normalized = scaler.transform(test_merge.iloc[:, :-1].values)\n",
    "\n",
    "n_components_pca = 12  # 주성분 개수\n",
    "pca = PCA(n_components=n_components_pca)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "train_merge_pca = pd.DataFrame(data=np.hstack((X_train_pca, train_merge['label'].values.reshape(-1, 1))),\n",
    "                               columns=[f'pca_{i}' for i in range(n_components_pca)] + ['label'])\n",
    "test_merge_pca = pd.DataFrame(data=np.hstack((X_test_pca, test_merge['label'].values.reshape(-1, 1))),\n",
    "                              columns=[f'pca_{i}' for i in range(n_components_pca)] + ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pca = CustomDataset(train_merge_pca)\n",
    "train_loader_pca = DataLoader(train_dataset_pca, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset_pca = CustomDataset(test_merge_pca)\n",
    "test_loader_pca = DataLoader(test_dataset_pca, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from resnet_1d import *\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)\n",
    "\n",
    "def create_models(num_classes=9):\n",
    "    model_18 = resnet18(num_classes=9, pretrained=False)\n",
    "    model_34 = resnet34(num_classes=9, pretrained=False)\n",
    "    \n",
    "    return model_18,model_34\n",
    "\n",
    "model_18, model_34 = create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "model = model_18\n",
    "\n",
    "# resnet18\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_18.parameters(), lr=2e-4)\n",
    "\n",
    "## train 및 valid\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader_pca)\n",
    "valid_loss = np.Inf  # train acc\n",
    "f1_ = 0  # val f1\n",
    "epoch_in = trange(100, desc='training')\n",
    "best_acc=0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in epoch_in:\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    \n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_dict in enumerate(train_loader_pca):\n",
    "\n",
    "        inputs = train_dict['data'].to(device).float()\n",
    "        inputs = inputs.reshape(64,1,1,12)\n",
    "        labels = train_dict['label'].to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        # if (batch_idx) % 1000 == 0:\n",
    "        #     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        #            .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "    \n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for test_dict in (test_loader_pca):\n",
    "            data_t = test_dict['data'].to(device).float()\n",
    "            data_t = data_t.reshape(64,1,1,12)\n",
    "            #print(data_t)\n",
    "            \n",
    "            target_t =  test_dict['label'].to(device).long()\n",
    "            #print(target_t)\n",
    "            \n",
    "            outputs_t = model(data_t)\n",
    "            \n",
    "            ####################### f1 score ################################\n",
    "            pred = outputs_t.argmax(dim=1).to(device)\n",
    "            target = target_t.view_as(pred).to(device)\n",
    "\n",
    "            preds_.append(pred)\n",
    "            targets_.append(target)\n",
    "            \n",
    "            # f1_score += f1(pred, target)\n",
    "            ##################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "\n",
    "        #print(total_t)\n",
    "\n",
    "        val_acc.append(100 * correct_t / total_t)\n",
    "        val_loss.append(batch_loss/len(test_loader_pca))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "\n",
    "        f1score = f1_score(targets_,preds_,  average='macro')\n",
    "        if best_acc < f1score:\n",
    "            best_acc = f1score\n",
    "            with open(\"./result/iot20d_pca+mfcc_res18_0112.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/iot20d_pca+mfcc_res18_0112.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
