{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "bin_data_ti = pd.read_csv(r\"F:\\Datasets\\Network_intrusion/network전처리.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공격 유형별로 데이터의 개수차이가 커서 같은 값으로 설정해줌 \n",
    "def sampling_func(data, n_sample, seed):\n",
    "    np.random.seed(seed) #실행할 때마다 동일한 샘플을 추출하기 위해 random seed 고정\n",
    "    N = len(data)\n",
    "    sample = data.take(np.random.permutation(N)[:n_sample])\n",
    "    return sample\n",
    "\n",
    "# valid dataset\n",
    "valid_df = bin_data_ti.groupby('label', group_keys=False).apply(sampling_func, n_sample=1000, seed=777)\n",
    "valid_df = valid_df.sort_index()\n",
    "\n",
    "bin_data_ti_1=bin_data_ti.drop(valid_df.index, axis=0)\n",
    "\n",
    "test_df = bin_data_ti_1.groupby('label', group_keys=False).apply(sampling_func, n_sample=1000, seed=7777)\n",
    "test_df = test_df.sort_index()\n",
    "\n",
    "train_df = bin_data_ti_1.drop(test_df.index, axis=0)\n",
    "\n",
    "train_df = train_df.groupby('label', group_keys=False).apply(sampling_func, n_sample=10000, seed=1004).reset_index(drop=True)\n",
    "\n",
    "## reset index 진행\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "print(valid_df['label'].value_counts())\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "zero_label_rows = train_df[train_df['label'] == 0] # 정상데이터 \n",
    "one_label_rows = train_df[train_df['label'] == 1] # 비정상 데이터 \n",
    "two_label_rows = train_df[train_df['label'] == 2] # 비정상 데이터 \n",
    "three_label_rows = train_df[train_df['label'] == 3] # 비정상 데이터 \n",
    "four_label_rows = train_df[train_df['label'] == 4] # 비정상 데이터 \n",
    "five_label_rows = train_df[train_df['label'] == 5] # 비정상 데이터 \n",
    "six_label_rows = train_df[train_df['label'] == 6] # 비정상 데이터 \n",
    "sev_label_rows = train_df[train_df['label'] == 7] # 비정상 데이터 \n",
    "eig_label_rows = train_df[train_df['label'] == 8] # 비정상 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행에 대해 MFCC 추출 및 새로운 데이터프레임에 저장\n",
    "import tqdm \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import tqdm \n",
    "\n",
    "mfcc_df = pd.DataFrame()\n",
    "zero_train_label_rows = four_label_rows.iloc[:,:-1]\n",
    "\n",
    "for index, row in tqdm.tqdm(zero_train_label_rows.iterrows()):\n",
    "    # 현재 행의 오디오 시그널\n",
    "    audio_signal = row.values\n",
    "    \n",
    "    # MFCC 추출 (여기에서는 13개의 계수만 추출)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=22050, n_mfcc=13)\n",
    "    \n",
    "    # 최적화된 코드\n",
    "    new_columns = {f'row_{index}_mfcc{i}': mfccs[i-1] for i in range(1, 14)}\n",
    "    mfcc_df = pd.concat([mfcc_df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "\n",
    "reshaped_array = mfcc_df.to_numpy().reshape((len(zero_train_label_rows), 13))\n",
    "reshaped_train_normal = pd.DataFrame(reshaped_array)\n",
    "reshaped_train_normal.to_csv('iot20d_reshaped_train_sample_4.csv', index=False)\n",
    "\n",
    "# 결과 출력\n",
    "print(reshaped_train_normal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_normal = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_normal.csv')\n",
    "train_one = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_1.csv')\n",
    "train_two = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_2.csv')\n",
    "train_three = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_3.csv')\n",
    "train_four = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_4.csv')\n",
    "train_five = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_5.csv')\n",
    "train_six = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_6.csv')\n",
    "train_seven = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_7.csv')\n",
    "train_eight = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_train_sample_8.csv')\n",
    "\n",
    "test_normal = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_normal.csv')\n",
    "test_one = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_1.csv')\n",
    "test_two = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_2.csv')\n",
    "test_three = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_3.csv')\n",
    "test_four = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_4.csv')\n",
    "test_five = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_5.csv')\n",
    "test_six = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_6.csv')\n",
    "test_seven = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_7.csv')\n",
    "test_eight = pd.read_csv('./IoTID20_mff전달/iot20d_reshaped_test_sample_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal['label']=0\n",
    "train_one['label']=1\n",
    "train_two['label']=2\n",
    "train_three['label']=3\n",
    "train_four['label']=4\n",
    "train_five['label']=5\n",
    "train_six['label']=6\n",
    "train_seven['label']=7\n",
    "train_eight['label']=8\n",
    "\n",
    "test_normal['label']=0\n",
    "test_one['label']=1\n",
    "test_two['label']=2\n",
    "test_three['label']=3\n",
    "test_four['label']=4\n",
    "test_five['label']=5\n",
    "test_six['label']=6\n",
    "test_seven['label']=7\n",
    "test_eight['label']=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge = pd.concat([train_normal, train_one, train_two, train_three, train_four, train_five, train_six, train_seven, train_eight], ignore_index=True)\n",
    "test_merge = pd.concat([test_normal, test_one, test_two, test_three, test_four, test_five, test_six, test_seven, test_eight], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(train_merge.iloc[:, :-1].values)\n",
    "X_test_normalized = scaler.transform(test_merge.iloc[:, :-1].values)\n",
    "\n",
    "n_components_pca = 12  # 주성분 개수\n",
    "pca = PCA(n_components=n_components_pca)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "train_merge_pca = pd.DataFrame(data=np.hstack((X_train_pca, train_merge['label'].values.reshape(-1, 1))),\n",
    "                               columns=[f'pca_{i}' for i in range(n_components_pca)] + ['label'])\n",
    "test_merge_pca = pd.DataFrame(data=np.hstack((X_test_pca, test_merge['label'].values.reshape(-1, 1))),\n",
    "                              columns=[f'pca_{i}' for i in range(n_components_pca)] + ['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
