{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC feature running code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# csv 파일로드 \n",
    "train_normal = pd.read_csv('./iot20d_reshaped_train_sample_normal.csv')\n",
    "train_one = pd.read_csv('./iot20d_reshaped_train_sample_1.csv')\n",
    "train_two = pd.read_csv('./iot20d_reshaped_train_sample_2.csv')\n",
    "train_three = pd.read_csv('./iot20d_reshaped_train_sample_3.csv')\n",
    "train_four = pd.read_csv('./iot20d_reshaped_train_sample_4.csv')\n",
    "train_five = pd.read_csv('./iot20d_reshaped_train_sample_5.csv')\n",
    "train_six = pd.read_csv('./iot20d_reshaped_train_sample_6.csv')\n",
    "train_seven = pd.read_csv('./iot20d_reshaped_train_sample_7.csv')\n",
    "train_eight = pd.read_csv('./iot20d_reshaped_train_sample_8.csv')\n",
    "\n",
    "test_normal = pd.read_csv('./iot20d_reshaped_test_sample_normal.csv')\n",
    "test_one = pd.read_csv('./iot20d_reshaped_test_sample_1.csv')\n",
    "test_two = pd.read_csv('./iot20d_reshaped_test_sample_2.csv')\n",
    "test_three = pd.read_csv('./iot20d_reshaped_test_sample_3.csv')\n",
    "test_four = pd.read_csv('./iot20d_reshaped_test_sample_4.csv')\n",
    "test_five = pd.read_csv('./iot20d_reshaped_test_sample_5.csv')\n",
    "test_six = pd.read_csv('./iot20d_reshaped_test_sample_6.csv')\n",
    "test_seven = pd.read_csv('./iot20d_reshaped_test_sample_7.csv')\n",
    "test_eight = pd.read_csv('./iot20d_reshaped_test_sample_8.csv')\n",
    "\n",
    "# 레이블 추가 \n",
    "train_normal['label']=0\n",
    "train_one['label']=1\n",
    "train_two['label']=2\n",
    "train_three['label']=3\n",
    "train_four['label']=4\n",
    "train_five['label']=5\n",
    "train_six['label']=6\n",
    "train_seven['label']=7\n",
    "train_eight['label']=8\n",
    "\n",
    "test_normal['label']=0\n",
    "test_one['label']=1\n",
    "test_two['label']=2\n",
    "test_three['label']=3\n",
    "test_four['label']=4\n",
    "test_five['label']=5\n",
    "test_six['label']=6\n",
    "test_seven['label']=7\n",
    "test_eight['label']=8\n",
    "\n",
    "train_merge = pd.concat([train_normal, train_one, train_two, train_three, train_four, train_five, train_six, train_seven, train_eight], ignore_index=True)\n",
    "test_merge = pd.concat([test_normal, test_one, test_two, test_three, test_four, test_five, test_six, test_seven, test_eight], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values  # 마지막 열을 제외한 데이터\n",
    "        self.labels = dataframe.iloc[:, -1].values  # 마지막 열을 라벨 데이터로 사용\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': torch.tensor(self.data[idx], dtype=torch.float32),\n",
    "                  'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_merge)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_merge)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from resnet_1d import *\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)\n",
    "\n",
    "def create_models(num_classes=9):\n",
    "    model_18 = resnet18(num_classes=9, pretrained=False)\n",
    "    model_34 = resnet34(num_classes=9, pretrained=False)\n",
    "    \n",
    "    return model_18,model_34\n",
    "\n",
    "model_18, model_34 = create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [64, 9]                   --\n",
       "├─Conv2d: 1-1                            [64, 64, 1, 7]            3,136\n",
       "├─BatchNorm2d: 1-2                       [64, 64, 1, 7]            128\n",
       "├─ReLU: 1-3                              [64, 64, 1, 7]            --\n",
       "├─MaxPool2d: 1-4                         [64, 64, 1, 4]            --\n",
       "├─Sequential: 1-5                        [64, 64, 1, 4]            --\n",
       "│    └─BasicBlock: 2-1                   [64, 64, 1, 4]            --\n",
       "│    │    └─Conv2d: 3-1                  [64, 64, 1, 4]            36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [64, 64, 1, 4]            128\n",
       "│    │    └─ReLU: 3-3                    [64, 64, 1, 4]            --\n",
       "│    │    └─Conv2d: 3-4                  [64, 64, 1, 4]            36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [64, 64, 1, 4]            128\n",
       "│    │    └─FloatFunctional: 3-6         --                        --\n",
       "│    │    └─ReLU: 3-7                    [64, 64, 1, 4]            --\n",
       "│    └─BasicBlock: 2-2                   [64, 64, 1, 4]            --\n",
       "│    │    └─Conv2d: 3-8                  [64, 64, 1, 4]            36,864\n",
       "│    │    └─BatchNorm2d: 3-9             [64, 64, 1, 4]            128\n",
       "│    │    └─ReLU: 3-10                   [64, 64, 1, 4]            --\n",
       "│    │    └─Conv2d: 3-11                 [64, 64, 1, 4]            36,864\n",
       "│    │    └─BatchNorm2d: 3-12            [64, 64, 1, 4]            128\n",
       "│    │    └─FloatFunctional: 3-13        --                        --\n",
       "│    │    └─ReLU: 3-14                   [64, 64, 1, 4]            --\n",
       "├─Sequential: 1-6                        [64, 128, 1, 2]           --\n",
       "│    └─BasicBlock: 2-3                   [64, 128, 1, 2]           --\n",
       "│    │    └─Conv2d: 3-15                 [64, 128, 1, 2]           73,728\n",
       "│    │    └─BatchNorm2d: 3-16            [64, 128, 1, 2]           256\n",
       "│    │    └─ReLU: 3-17                   [64, 128, 1, 2]           --\n",
       "│    │    └─Conv2d: 3-18                 [64, 128, 1, 2]           147,456\n",
       "│    │    └─BatchNorm2d: 3-19            [64, 128, 1, 2]           256\n",
       "│    │    └─Sequential: 3-20             [64, 128, 1, 2]           8,448\n",
       "│    │    └─FloatFunctional: 3-21        --                        --\n",
       "│    │    └─ReLU: 3-22                   [64, 128, 1, 2]           --\n",
       "│    └─BasicBlock: 2-4                   [64, 128, 1, 2]           --\n",
       "│    │    └─Conv2d: 3-23                 [64, 128, 1, 2]           147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [64, 128, 1, 2]           256\n",
       "│    │    └─ReLU: 3-25                   [64, 128, 1, 2]           --\n",
       "│    │    └─Conv2d: 3-26                 [64, 128, 1, 2]           147,456\n",
       "│    │    └─BatchNorm2d: 3-27            [64, 128, 1, 2]           256\n",
       "│    │    └─FloatFunctional: 3-28        --                        --\n",
       "│    │    └─ReLU: 3-29                   [64, 128, 1, 2]           --\n",
       "├─Sequential: 1-7                        [64, 256, 1, 1]           --\n",
       "│    └─BasicBlock: 2-5                   [64, 256, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-30                 [64, 256, 1, 1]           294,912\n",
       "│    │    └─BatchNorm2d: 3-31            [64, 256, 1, 1]           512\n",
       "│    │    └─ReLU: 3-32                   [64, 256, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-33                 [64, 256, 1, 1]           589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [64, 256, 1, 1]           512\n",
       "│    │    └─Sequential: 3-35             [64, 256, 1, 1]           33,280\n",
       "│    │    └─FloatFunctional: 3-36        --                        --\n",
       "│    │    └─ReLU: 3-37                   [64, 256, 1, 1]           --\n",
       "│    └─BasicBlock: 2-6                   [64, 256, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-38                 [64, 256, 1, 1]           589,824\n",
       "│    │    └─BatchNorm2d: 3-39            [64, 256, 1, 1]           512\n",
       "│    │    └─ReLU: 3-40                   [64, 256, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-41                 [64, 256, 1, 1]           589,824\n",
       "│    │    └─BatchNorm2d: 3-42            [64, 256, 1, 1]           512\n",
       "│    │    └─FloatFunctional: 3-43        --                        --\n",
       "│    │    └─ReLU: 3-44                   [64, 256, 1, 1]           --\n",
       "├─Sequential: 1-8                        [64, 512, 1, 1]           --\n",
       "│    └─BasicBlock: 2-7                   [64, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-45                 [64, 512, 1, 1]           1,179,648\n",
       "│    │    └─BatchNorm2d: 3-46            [64, 512, 1, 1]           1,024\n",
       "│    │    └─ReLU: 3-47                   [64, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-48                 [64, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-49            [64, 512, 1, 1]           1,024\n",
       "│    │    └─Sequential: 3-50             [64, 512, 1, 1]           132,096\n",
       "│    │    └─FloatFunctional: 3-51        --                        --\n",
       "│    │    └─ReLU: 3-52                   [64, 512, 1, 1]           --\n",
       "│    └─BasicBlock: 2-8                   [64, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-53                 [64, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-54            [64, 512, 1, 1]           1,024\n",
       "│    │    └─ReLU: 3-55                   [64, 512, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-56                 [64, 512, 1, 1]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-57            [64, 512, 1, 1]           1,024\n",
       "│    │    └─FloatFunctional: 3-58        --                        --\n",
       "│    │    └─ReLU: 3-59                   [64, 512, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [64, 512, 1, 1]           --\n",
       "├─Linear: 1-10                           [64, 9]                   4,617\n",
       "==========================================================================================\n",
       "Total params: 11,174,857\n",
       "Trainable params: 11,174,857\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 778.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 6.75\n",
       "Params size (MB): 44.70\n",
       "Estimated Total Size (MB): 51.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model_18, input_size = (64,1,1,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4955d4b63e4ea28716a49274760ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.4786, train acc: 73.9943\n",
      "\n",
      "train loss: 0.4606, train acc: 74.8911\n",
      "\n",
      "train loss: 0.4527, train acc: 74.9756\n",
      "\n",
      "train loss: 0.4480, train acc: 75.2300\n",
      "\n",
      "train loss: 0.4439, train acc: 75.2400\n",
      "\n",
      "train loss: 0.4410, train acc: 75.3301\n",
      "\n",
      "train loss: 0.4387, train acc: 75.4312\n",
      "\n",
      "train loss: 0.4365, train acc: 75.6479\n",
      "\n",
      "train loss: 0.4345, train acc: 75.8368\n",
      "\n",
      "train loss: 0.4329, train acc: 75.8879\n",
      "\n",
      "train loss: 0.4316, train acc: 75.9179\n",
      "\n",
      "train loss: 0.4304, train acc: 75.8290\n",
      "\n",
      "train loss: 0.4292, train acc: 76.1091\n",
      "\n",
      "train loss: 0.4281, train acc: 75.9446\n",
      "\n",
      "train loss: 0.4271, train acc: 75.9957\n",
      "\n",
      "train loss: 0.4261, train acc: 76.1669\n",
      "\n",
      "train loss: 0.4254, train acc: 76.1947\n",
      "\n",
      "train loss: 0.4246, train acc: 76.3158\n",
      "\n",
      "train loss: 0.4237, train acc: 76.5236\n",
      "\n",
      "train loss: 0.4231, train acc: 76.1513\n",
      "\n",
      "train loss: 0.4224, train acc: 76.3547\n",
      "\n",
      "train loss: 0.4218, train acc: 76.3458\n",
      "\n",
      "train loss: 0.4212, train acc: 76.3025\n",
      "\n",
      "train loss: 0.4206, train acc: 76.3891\n",
      "\n",
      "train loss: 0.4201, train acc: 76.4747\n",
      "\n",
      "train loss: 0.4195, train acc: 76.4680\n",
      "\n",
      "train loss: 0.4190, train acc: 76.4058\n",
      "\n",
      "train loss: 0.4186, train acc: 76.5014\n",
      "\n",
      "train loss: 0.4181, train acc: 76.5136\n",
      "\n",
      "train loss: 0.4177, train acc: 76.4002\n",
      "\n",
      "train loss: 0.4173, train acc: 76.4458\n",
      "\n",
      "train loss: 0.4169, train acc: 76.5125\n",
      "\n",
      "train loss: 0.4164, train acc: 76.5681\n",
      "\n",
      "train loss: 0.4160, train acc: 76.5758\n",
      "\n",
      "train loss: 0.4157, train acc: 76.6103\n",
      "\n",
      "train loss: 0.4153, train acc: 76.5358\n",
      "\n",
      "train loss: 0.4149, train acc: 76.6770\n",
      "\n",
      "train loss: 0.4146, train acc: 76.5714\n",
      "\n",
      "train loss: 0.4143, train acc: 76.7336\n",
      "\n",
      "train loss: 0.4139, train acc: 76.8014\n",
      "\n",
      "train loss: 0.4136, train acc: 76.6570\n",
      "\n",
      "train loss: 0.4133, train acc: 76.6736\n",
      "\n",
      "train loss: 0.4130, train acc: 76.8025\n",
      "\n",
      "train loss: 0.4127, train acc: 76.6592\n",
      "\n",
      "train loss: 0.4125, train acc: 76.7648\n",
      "\n",
      "train loss: 0.4122, train acc: 76.9181\n",
      "\n",
      "train loss: 0.4119, train acc: 76.8559\n",
      "\n",
      "train loss: 0.4116, train acc: 76.9637\n",
      "\n",
      "train loss: 0.4113, train acc: 76.7792\n",
      "\n",
      "train loss: 0.4111, train acc: 76.9237\n",
      "\n",
      "train loss: 0.4108, train acc: 76.8059\n",
      "\n",
      "train loss: 0.4106, train acc: 76.8314\n",
      "\n",
      "train loss: 0.4104, train acc: 76.8548\n",
      "\n",
      "train loss: 0.4101, train acc: 76.9726\n",
      "\n",
      "train loss: 0.4099, train acc: 76.7781\n",
      "\n",
      "train loss: 0.4097, train acc: 76.9181\n",
      "\n",
      "train loss: 0.4095, train acc: 76.8570\n",
      "\n",
      "train loss: 0.4093, train acc: 76.8803\n",
      "\n",
      "train loss: 0.4090, train acc: 76.7859\n",
      "\n",
      "train loss: 0.4088, train acc: 76.9281\n",
      "\n",
      "train loss: 0.4087, train acc: 76.9170\n",
      "\n",
      "train loss: 0.4084, train acc: 76.9837\n",
      "\n",
      "train loss: 0.4083, train acc: 76.9003\n",
      "\n",
      "train loss: 0.4081, train acc: 77.0292\n",
      "\n",
      "train loss: 0.4079, train acc: 77.0504\n",
      "\n",
      "train loss: 0.4077, train acc: 76.9737\n",
      "\n",
      "train loss: 0.4075, train acc: 77.0526\n",
      "\n",
      "train loss: 0.4073, train acc: 77.1404\n",
      "\n",
      "train loss: 0.4071, train acc: 77.1248\n",
      "\n",
      "train loss: 0.4070, train acc: 77.1693\n",
      "\n",
      "train loss: 0.4068, train acc: 76.9815\n",
      "\n",
      "train loss: 0.4066, train acc: 77.1659\n",
      "\n",
      "train loss: 0.4065, train acc: 77.0159\n",
      "\n",
      "train loss: 0.4063, train acc: 77.2371\n",
      "\n",
      "train loss: 0.4061, train acc: 77.0393\n",
      "\n",
      "train loss: 0.4060, train acc: 77.1759\n",
      "\n",
      "train loss: 0.4058, train acc: 77.1615\n",
      "\n",
      "train loss: 0.4057, train acc: 77.3104\n",
      "\n",
      "train loss: 0.4055, train acc: 77.2582\n",
      "\n",
      "train loss: 0.4053, train acc: 77.1448\n",
      "\n",
      "train loss: 0.4052, train acc: 77.1459\n",
      "\n",
      "train loss: 0.4050, train acc: 77.2093\n",
      "\n",
      "train loss: 0.4049, train acc: 77.2271\n",
      "\n",
      "train loss: 0.4048, train acc: 77.1671\n",
      "\n",
      "train loss: 0.4046, train acc: 77.2226\n",
      "\n",
      "train loss: 0.4045, train acc: 77.1859\n",
      "\n",
      "train loss: 0.4044, train acc: 77.3349\n",
      "\n",
      "train loss: 0.4042, train acc: 77.3504\n",
      "\n",
      "train loss: 0.4041, train acc: 77.4482\n",
      "\n",
      "train loss: 0.4039, train acc: 77.1326\n",
      "\n",
      "train loss: 0.4038, train acc: 77.3826\n",
      "\n",
      "train loss: 0.4037, train acc: 77.2771\n",
      "\n",
      "train loss: 0.4036, train acc: 77.1815\n",
      "\n",
      "train loss: 0.4035, train acc: 77.2348\n",
      "\n",
      "train loss: 0.4033, train acc: 77.4938\n",
      "\n",
      "train loss: 0.4032, train acc: 77.4015\n",
      "\n",
      "train loss: 0.4031, train acc: 77.3371\n",
      "\n",
      "train loss: 0.4030, train acc: 77.2760\n",
      "\n",
      "train loss: 0.4029, train acc: 77.4793\n",
      "\n",
      "train loss: 0.4027, train acc: 77.3638\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "model = model_18\n",
    "\n",
    "# resnet18\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_18.parameters(), lr=2e-4)\n",
    "\n",
    "## train 및 valid\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "valid_loss = np.Inf  # train acc\n",
    "f1_ = 0  # val f1\n",
    "epoch_in = trange(100, desc='training')\n",
    "best_acc=0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in epoch_in:\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    \n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_dict in enumerate(train_loader):\n",
    "\n",
    "        inputs = train_dict['data'].to(device).float()\n",
    "        inputs = inputs.reshape(64,1,1,13)\n",
    "        labels = train_dict['label'].to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        # if (batch_idx) % 1000 == 0:\n",
    "        #     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        #            .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "    \n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for test_dict in (test_loader):\n",
    "            data_t = test_dict['data'].to(device).float()\n",
    "            data_t = data_t.reshape(64,1,1,13)\n",
    "            #print(data_t)\n",
    "            \n",
    "            target_t =  test_dict['label'].to(device).long()\n",
    "            #print(target_t)\n",
    "            \n",
    "            outputs_t = model(data_t)\n",
    "            \n",
    "            ####################### f1 score ################################\n",
    "            pred = outputs_t.argmax(dim=1).to(device)\n",
    "            target = target_t.view_as(pred).to(device)\n",
    "\n",
    "            preds_.append(pred)\n",
    "            targets_.append(target)\n",
    "            \n",
    "            # f1_score += f1(pred, target)\n",
    "            ##################################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "\n",
    "        #print(total_t)\n",
    "\n",
    "        val_acc.append(100 * correct_t / total_t)\n",
    "        val_loss.append(batch_loss/len(test_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "\n",
    "        f1score = f1_score(targets_,preds_,  average='macro')\n",
    "        if best_acc < f1score:\n",
    "            best_acc = f1score\n",
    "            with open(\"./result/iot20d_mfcc_res18_0108.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/iot20d_mfcc_res18_0108.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintorch1221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
